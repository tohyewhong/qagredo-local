# QAGRedo Offline Server Guide

> **This is your single reference for working on the offline server.**
> Everything you need is inside this `qagredo_host/` directory.

---

## What QAGRedo does

QAGRedo is an automated pipeline that:

1. **Reads** your input documents (JSONL format)
2. **Generates complex questions** using 10 question types (analysis,
   aggregation, comparison, inference, causal, temporal, multi-hop,
   synthesis, evaluation, counterfactual)
3. **Generates grounded answers** with supporting evidence, using a
   structured format and low temperature (0.3) for factual accuracy
4. **Verifies grounding** using a hybrid method: fast semantic similarity
   (MiniLM) for clear cases, LLM-as-judge fallback for edge cases
   (counting, aggregation, inference). Judging uses a **separate model**
   (Qwen) to avoid self-evaluation bias.
5. **Grades** each document (A/B/C/D/F) based on answer grounding confidence
6. **Saves** results to timestamped folders with detailed reasons for any
   ungrounded content

For full algorithm details and design rationale, see `docs/ALGORITHM_REPORT.md`.

---

## Directory map

```
qagredo_host/                          <-- YOU ARE HERE
|
|-- OFFLINE_GUIDE.md                   * This file
|-- run.sh                             * Run the pipeline
|-- jupyter.sh                         * Start Jupyter Lab
|-- setup_offline.sh                     First-time setup only
|
|-- config/
|   +-- config.yaml                    * Pipeline configuration (edit this)
|
|-- data/
|   +-- *.jsonl                        * Your input documents (put files here)
|
|-- output/                            * Results appear here (auto-created)
|   +-- vllm/<model>/YYYY-MM-DD_HHMMSS/
|       +-- *_analysis.json
|
|-- utils/                               Python modules (edit to customise)
|   |-- question_generator.py            10 question types, few-shot examples
|   |-- answer_generator.py              Structured answers, 3 retries
|   |-- hallucination_checker.py         Hybrid grading (semantic + LLM)
|   |-- output_manager.py                Timestamped output folders
|   +-- ...
|
|-- run_qa_pipeline.py                   Main pipeline script
|-- docker-compose.offline.yml           Docker services definition
|-- scripts/
|   |-- conversion/
|   |   +-- convert_to_qagredo_jsonl.py  Convert JSON/PDF/TXT/XLSX -> JSONL
|   +-- utils/
|       +-- summarize_run.sh             Summarise results with reasons
|
|-- docs/                                Detailed documentation
|   |-- ALGORITHM_REPORT.md              Algorithm details & design rationale
|   |-- OFFLINE_SETUP_GUIDE.md            5-file offline deployment guide
|   +-- architecture/
|
|-- README.md                            Project overview
|-- .env                                 Auto-generated by setup (do not edit)
|-- hf_cache/                            Model cache for vLLM (auto-created)
|-- hf_cache_judge/                     Model cache for vLLM-judge (auto-created)
+-- models_llm/ -> (symlink to LLM)     Created by setup_offline.sh
```

Files marked with * are the ones you interact with most often.

---

## Quick command reference

All commands are run from inside `qagredo_host/`:

```bash
cd /path/to/qagredo_host
```

### Run the pipeline

```bash
bash run.sh                         # Start three containers (vLLM + vLLM-judge + pipeline)
bash run.sh --down                  # Stop all containers
bash run.sh --status                # Show container status + vLLM health
bash run.sh --logs                  # Tail vLLM logs (Ctrl+C to stop)
bash run.sh --show-config           # Display config, env vars, and data files
bash run.sh --help                  # Show all options
```

### Summarise results

```bash
bash run.sh --summarize --latest              # Summarise latest run
bash run.sh --summarize --latest --json       # Save summary as JSON
bash run.sh --summarize --all                 # Summarise all runs
```

### Convert input files

```bash
# Convert JSON / PDF / TXT / XLSX to JSONL
python3 scripts/conversion/convert_to_qagredo_jsonl.py \
  --input data/my_input.json \
  --output data/my_input.jsonl

# Or via run.sh:
bash run.sh --convert --input data/my_input.json --output data/my_input.jsonl
```

### Jupyter Lab

```bash
bash jupyter.sh                     # Start Jupyter + vLLM
bash jupyter.sh --no-vllm           # Start Jupyter only (no GPU)
bash jupyter.sh --down              # Stop Jupyter
```

Then open: `http://localhost:8899`

If connecting remotely:
```bash
ssh -L 8899:localhost:8899 user@offline-server
```

---

## Day-to-day workflow

### 1. Put your data in `data/`

Copy your input files (JSON or JSONL) into the `data/` folder:

```bash
cp /path/to/my_documents.jsonl data/
```

If you have JSON files that need conversion:
```bash
python3 scripts/conversion/convert_to_qagredo_jsonl.py \
    --input data/my_input.json \
    --output data/my_input.jsonl
```

### 2. Edit `config/config.yaml`

Open the config file and set your input file and parameters:

```bash
vi config/config.yaml
# or: nano config/config.yaml
```

Key settings to change:

```yaml
run:
  input_file: my_input.jsonl        # <-- your file in data/
  num_documents: 5                  # <-- how many documents to process

question_generation:
  num_questions: 3                  # <-- questions per document
  complexity: "advanced"            # <-- basic, moderate, or advanced

answer_generation:
  temperature: 0.3                  # <-- lower = more factual

hallucination:
  method: "hybrid"                  # <-- semantic, keyword, llm, or hybrid
```

### 3. Run the pipeline

```bash
bash run.sh
```

This will:
1. Start vLLM (GPU) and wait for it to be ready
2. Run the QAGRedo pipeline
3. Save results in `output/vllm/<model>/YYYY-MM-DD_HHMMSS/`

Each run creates a **unique timestamped folder** (date + time), so multiple
runs per day do not overwrite each other.

### 4. View results

```bash
# Quick summary (text output)
bash scripts/utils/summarize_run.sh --latest

# Save summary as JSON (for detailed analysis)
bash scripts/utils/summarize_run.sh --latest --json

# List output files
ls -lt output/vllm/*/
```

The terminal summary shows Generator, Judge, and Provider. The **run_summary.json** includes:
- `generator_model` and `judge_model` (separate fields)
- Per-document statistics (grade, confidence, grounded/ungrounded counts)
- Per-QA details with grounding method and confidence
- **For ungrounded answers**: specific reasons, ungrounded sentences, and
  Qwen judge verdict
- **Ungrounded highlights**: flat list of all failed QA pairs across documents
  for quick scanning

### 5. Re-run with different settings

Just edit `config/config.yaml` and run again:

```bash
vi config/config.yaml
bash run.sh
```

No need to restart Docker or rebuild anything. Changes are picked up
automatically. Each run goes to a new timestamped folder.

---

## Pipeline details

### Question generation (10 types)

| Type | What it tests |
|------|---------------|
| Analysis | Break down information into parts |
| Aggregation | Count/sum across document |
| Comparison | Compare/contrast entities |
| Inference | Draw conclusions from facts |
| Causal | Cause-and-effect relationships |
| Temporal | Timeline and sequence |
| Multi-hop | Connect multiple separate facts |
| Synthesis | Combine 3+ facts into analysis |
| Evaluation | Assess strength of claims/evidence |
| Counterfactual | Reason about hypothetical changes |

The `advanced` preset (default) uses all 10 types. Each question must require
reasoning across at least 2 different parts of the document.

### Answer generation (structured + retries)

- **Structured format**: Answer + Supporting Evidence
- **"List then count"**: improves aggregation accuracy by ~30%
- **Low temperature** (0.3): suppresses creative hallucination
- **3 retries**: if an answer fails grounding, it is regenerated up to 3 times

### Hallucination grading (hybrid)

1. **Pass 1 (fast)**: Semantic similarity with sliding window (MiniLM, CPU)
   - Compares each answer sentence against 1/2/3-sentence document chunks
   - If all grounded: done (no LLM call needed)
2. **Pass 2 (accurate)**: LLM-as-judge (Qwen, only if Pass 1 found ungrounded sentences)
   - Uses a **separate model** (Qwen2.5-7B) to avoid self-evaluation bias
   - Handles counting, aggregation, inference, multi-hop reasoning
   - Can override or confirm the semantic verdict

### Grading scale

| Grade | Confidence | Meaning |
|-------|-----------|---------|
| A | >= 90% | Excellent -- answers well-grounded in document |
| B | >= 80% | Good -- mostly grounded |
| C | >= 70% | Fair -- some ungrounded claims |
| D | >= 60% | Poor -- significant grounding issues |
| F | < 60% | Fail -- mostly ungrounded |

---

## Configuration reference

### `config/config.yaml` -- all settings

```yaml
# What to process
run:
  input_file: dev-data.jsonl        # file in data/ folder
  num_documents: 2                  # how many to process

# LLM connection (usually don't need to change)
llm:
  provider: "vllm"
  model: "meta-llama/Meta-Llama-3.1-8B-Instruct"
  temperature: 0.7                  # for question generation
  max_tokens: 500
  api_key: "llama-local"
  base_url: "http://localhost:8100/v1"

# Answer generation
answer_generation:
  temperature: 0.3                  # lower = more deterministic/factual
  multi_turn:
    enable_rejection: true
    min_confidence_threshold: 0.7
    max_regeneration_attempts: 3    # up to 3 retries for answers

# Question generation
question_generation:
  num_questions: 3
  complexity: "advanced"            # basic | moderate | advanced
  duplicate_similarity_threshold: 0.85
  deduplication_method: "semantic"
  validation:
    enable_rejection: true
    min_confidence_threshold: 0.7
    max_regeneration_attempts: 2
    method: "semantic"

# Hallucination checking
hallucination:
  method: "hybrid"                  # semantic | keyword | llm | hybrid
```

### Environment variables (optional overrides)

Set these in your shell before running, or add to `.env`:

| Variable | Default | What it does |
|----------|---------|-------------|
| `VLLM_MAX_MODEL_LEN` | `8192` | Max context window |
| `VLLM_TP_SIZE` | `2` | Tensor parallelism (number of GPUs) |
| `VLLM_GPU_UTIL` | `0.85` | GPU memory fraction to use |
| `VLLM_MODEL` | `/models/Meta-Llama-3.1-8B-Instruct` | Model path inside container |
| `VLLM_SERVED_MODEL_NAME` | `meta-llama/Meta-Llama-3.1-8B-Instruct` | Model name for API |
| `VLLM_API_KEY` | `llama-local` | API key (must match config.yaml) |
| `VLLM_JUDGE_MODEL` | `/models/Qwen2.5-7B-Instruct` | Judge model path (vLLM-judge container) |
| `VLLM_JUDGE_SERVED_NAME` | `Qwen/Qwen2.5-7B-Instruct` | Judge model name for API |
| `VLLM_JUDGE_API_KEY` | `qwen-local` | Judge API key (must match hallucination config) |

**Models & hardware**: The system uses **two LLMs** â€” Llama-3.1-8B (vLLM, GPU 0) for
question/answer generation, and Qwen2.5-7B (vLLM-judge, GPU 1) for independent
hallucination grading. Using a separate judge model avoids self-evaluation bias.

Example:
```bash
export VLLM_MAX_MODEL_LEN=16384
bash run.sh
```

---

## Changing models / Pointing to a remote LLM server

If your LLMs (Llama, Qwen, or any other models) are already running on a
**different machine** (e.g. "Server A"), you do not need to start vLLM locally.
You only need to tell QAGRedo where to find those models.

Everything is controlled by **one file**: `config/config.yaml`.

### What you need to know before editing

You need **three pieces of information** for each model:

| Info | How to get it | Example |
|------|---------------|---------|
| **IP address** (or hostname) of Server A | Ask your admin, or run `hostname -I` on Server A | `192.168.1.50` |
| **Port** the model is running on | Check Server A's vLLM startup command or ask your admin | `8100` (generator), `8101` (judge) |
| **Served model name** | Run the curl command below against Server A | `meta-llama/Meta-Llama-3.1-8B-Instruct` |

To find the exact model name that Server A is serving:

```bash
# Generator model (replace IP and port with Server A's values)
curl http://192.168.1.50:8100/v1/models

# Judge model
curl http://192.168.1.50:8101/v1/models
```

The response will look like:

```json
{
  "data": [
    {
      "id": "meta-llama/Meta-Llama-3.1-8B-Instruct",
      "object": "model"
    }
  ]
}
```

The `"id"` value is what you put in the `model:` field.

### Step-by-step: Point QAGRedo at Server A

**1. Open the config file:**

```bash
vi config/config.yaml
# or: nano config/config.yaml
```

**2. Edit the `llm:` section** (generator -- produces questions and answers):

Change `base_url` to Server A's IP and port, and `model` to match what
Server A is serving:

```yaml
llm:
  provider: "vllm"
  model: "meta-llama/Meta-Llama-3.1-8B-Instruct"   # must match Server A's served name
  base_url: "http://192.168.1.50:8100/v1"           # Server A's IP and port
  api_key: "llama-local"                             # must match Server A's API key
  temperature: 0.7
  max_tokens: 500
  max_retries: 3
  retry_delay: 1.0
  timeout: 60
```

**3. Edit the `judge:` section** (judge -- checks for hallucinations):

```yaml
judge:
  provider: "vllm"
  model: "Qwen/Qwen2.5-7B-Instruct"                # must match Server A's served name
  base_url: "http://192.168.1.50:8101/v1"           # Server A's IP and port
  api_key: "qwen-local"                              # must match Server A's API key
  temperature: 0.0
  max_tokens: 200
  timeout: 60
  max_retries: 3
  retry_delay: 1.0
```

**4. Save and run:**

```bash
bash run.sh
```

> **Note:** If Server A's models are already running, you do NOT need to start
> local vLLM containers. You can run just the pipeline container, or run the
> pipeline directly on the host:
>
> ```bash
> .venv/bin/python run_qa_pipeline.py --config config/config.yaml
> ```

### Verify connectivity before running

Always check that you can reach Server A's models before starting the pipeline:

```bash
# Check generator health
curl -i http://192.168.1.50:8100/health

# Check judge health
curl -i http://192.168.1.50:8101/health

# List available generator models
curl -s http://192.168.1.50:8100/v1/models | python3 -m json.tool

# List available judge models
curl -s http://192.168.1.50:8101/v1/models | python3 -m json.tool
```

All four should succeed before you run the pipeline. If any fail, check:
- Is Server A's vLLM actually running? (`bash run.sh --status` on Server A)
- Is there a firewall blocking the port? (`telnet 192.168.1.50 8100`)
- Is the IP correct? (`ping 192.168.1.50`)

### Swapping to completely different models (Model B and Model C)

If Server A is running different models (not Llama and Qwen), the process is
the same -- you only need to change the `model` and possibly `base_url` fields.

**Example:** Server A runs "Model-B" on port 9000 and "Model-C" on port 9001:

**Step 1.** Find the exact served model names:

```bash
curl -s http://192.168.1.50:9000/v1/models | python3 -m json.tool
# Returns:  "id": "org/Model-B-70B-Instruct"

curl -s http://192.168.1.50:9001/v1/models | python3 -m json.tool
# Returns:  "id": "org/Model-C-32B-Instruct"
```

**Step 2.** Edit `config/config.yaml`:

```yaml
llm:
  provider: "vllm"
  model: "org/Model-B-70B-Instruct"                 # <-- from /v1/models
  base_url: "http://192.168.1.50:9000/v1"           # <-- Server A IP + port
  api_key: "server-a-api-key"                        # <-- whatever Server A expects
  temperature: 0.7
  max_tokens: 500
  max_retries: 3
  retry_delay: 1.0
  timeout: 60

judge:
  provider: "vllm"
  model: "org/Model-C-32B-Instruct"                  # <-- from /v1/models
  base_url: "http://192.168.1.50:9001/v1"            # <-- Server A IP + port
  api_key: "server-a-api-key"                         # <-- whatever Server A expects
  temperature: 0.0
  max_tokens: 200
  timeout: 60
  max_retries: 3
  retry_delay: 1.0
```

**Step 3.** Save and run:

```bash
bash run.sh
```

Output will be saved to `output/vllm/org-model-b-70b-instruct/YYYY-MM-DD_HHMMSS/`
(folder name is derived from the generator model name).

### What each field means

| Field | Section | Purpose | How to decide the value |
|-------|---------|---------|------------------------|
| `provider` | `llm` / `judge` | Which API protocol to use | Always `"vllm"` for local/remote vLLM servers |
| `model` | `llm` / `judge` | Model name sent in API requests | **Must exactly match** what `/v1/models` returns |
| `base_url` | `llm` / `judge` | Full URL to the vLLM API | `http://<IP>:<PORT>/v1` |
| `api_key` | `llm` / `judge` | Authentication token | Must match what vLLM was started with (`--api-key`) |
| `temperature` | `llm` | Randomness for generation | `0.7` for questions (diverse), `0.3` for answers (factual) |
| `temperature` | `judge` | Randomness for grading | Always `0.0` (deterministic, reproducible verdicts) |
| `max_tokens` | `llm` / `judge` | Max output length per request | `500` for generation, `200` for judge |
| `timeout` | `llm` / `judge` | Seconds before request times out | Increase if models are slow (e.g. large models, busy server) |
| `max_retries` | `llm` / `judge` | Retry count on API failure | `3` is a safe default |
| `retry_delay` | `llm` / `judge` | Seconds between retries | `1.0` is a safe default |

### Common scenarios

| Scenario | What to change |
|----------|----------------|
| Models on Server A, same ports (8100/8101) | Change `base_url` IP only (keep ports) |
| Models on Server A, different ports | Change `base_url` IP and ports |
| Different model names | Change `model` to match `/v1/models` output |
| Different API key on Server A | Change `api_key` to match Server A's key |
| Both models on the same port (single server) | Set both `llm.base_url` and `judge.base_url` to the same URL, but use different `model` names |
| Model is slow / times out | Increase `timeout` (e.g. `120` or `300`) |
| Want to use the same model for generation and judging | Set `judge.model` and `judge.base_url` to match `llm` (not recommended -- loses self-evaluation bias protection) |

### Troubleshooting model changes

| Problem | Cause | Fix |
|---------|-------|-----|
| `Connection refused` | Server A's vLLM not running, or wrong IP/port | Verify with `curl http://<IP>:<PORT>/health` |
| `404 Not Found` on `/v1/chat/completions` | Wrong `base_url` (missing `/v1` suffix) | Ensure `base_url` ends with `/v1` |
| `Model not found` | `model` in config doesn't match served name | Run `curl http://<IP>:<PORT>/v1/models` and copy the exact `id` |
| `401 Unauthorized` | `api_key` doesn't match Server A's key | Ask admin for the correct key, or check Server A's `--api-key` flag |
| `timeout` errors | Model is overloaded or very large | Increase `timeout` to `120` or `300` |
| Grading always uses semantic (never LLM judge) | Judge `base_url` is unreachable | Check `judge.base_url` separately from `llm.base_url` |
| Output folder has wrong model name | Config was not saved before running | Re-check `config/config.yaml` and re-run |

---

## Understanding the output

### Output folder structure

Each run creates a unique timestamped folder:

```
output/vllm/meta-llama-meta-llama-3.1-8b-instruct/2026-02-13_143025/
|-- 20260213_143025_doc_abc_1_doc1_analysis.json
|-- 20260213_143210_doc_abc_2_doc2_analysis.json
+-- ...
```

### Per-document analysis JSON

Each file contains:

| Section | Content |
|---------|---------|
| `document` | Source document metadata (id, title, content) |
| `qa_pairs` | Generated questions + answers with per-pair grounding |
| `qa_pairs[].grading` | `is_grounded`, `confidence`, `method`, `issues`, `ungrounded_sentences` |
| `qa_pairs[].grading.llm_verdict` | Qwen judge verdict and reason (if hybrid/LLM was used) |
| `supporting_evidence` | Quotes from the document supporting each answer |
| `grading_summary` | Overall grade (A-F), confidence, method, `judge_model` |
| `question_generation` | Model, timestamp, generation metadata |
| `answer_generation` | Model, timestamp, generation metadata |

### Run summary (for analysts)

```bash
# Text summary to terminal
bash scripts/utils/summarize_run.sh --latest

# Save as JSON for detailed analysis
bash scripts/utils/summarize_run.sh --latest --json
```

The JSON summary includes:
- **`generator_model` and `judge_model`** (separate fields)
- **Per-document statistics** and per-QA details
- **Ungrounded reasons**: for each ungrounded answer, the specific issues,
  ungrounded sentences, and Qwen verdict with explanation
- **Ungrounded highlights**: quick-scan list of all problems across all documents

---

## Troubleshooting

### "Permission denied" when copying files or saving summaries

The pipeline container runs as **your user** (not root) -- output files are
owned by you automatically. The entrypoint and post-run scripts use
`--privileged --userns=host` to fix any root-owned files created by Docker.

If you encounter permission issues with old files:

```bash
# Re-run setup (fixes permissions automatically)
bash setup_offline.sh --force
```

If you cannot delete `hf_cache` or `hf_cache_judge` files (created by vLLM as root):

```bash
# Fix vLLM generator cache
docker run --rm --privileged --userns=host -u 0 --entrypoint bash \
  -v "$(pwd)/hf_cache:/hf" vllm/vllm-openai:v0.5.3.post1 \
  -c "rm -rf /hf/modules /hf/hub"

# Fix vLLM-judge cache
docker run --rm --privileged --userns=host -u 0 --entrypoint bash \
  -v "$(pwd)/hf_cache_judge:/hf" vllm/vllm-openai:v0.5.3.post1 \
  -c "rm -rf /hf/modules /hf/hub"
```

### "Context length exceeded" error

Increase the max model length:
```bash
export VLLM_MAX_MODEL_LEN=16384
bash run.sh
```

### vLLM not starting or GPU errors

```bash
# Check vLLM logs
bash run.sh --logs

# Check GPU status
nvidia-smi

# Restart everything
bash run.sh --down
bash run.sh
```

**`pynvml.NVMLError_InvalidArgument`**: Do **not** set `CUDA_VISIBLE_DEVICES` in
docker-compose. GPU assignment is handled entirely by Docker's
`deploy.resources.reservations.devices.device_ids`. Docker maps the reserved GPU
as device 0 inside the container, so `CUDA_VISIBLE_DEVICES: "1"` would try to
find a non-existent second GPU.

### Wrong model or 404 errors

Make sure `VLLM_SERVED_MODEL_NAME` matches `llm.model` in `config/config.yaml`:

```bash
# Check current values
grep "model:" config/config.yaml
echo $VLLM_SERVED_MODEL_NAME
```

### Pipeline seems stuck

```bash
# Check container status
bash run.sh --status

# Check if vLLM is healthy
curl http://localhost:8100/health
```

---

## Updating code from the dev machine

When you receive a new `qagredo_bundle.tar.gz`:

```bash
# Go to the parent directory (staging area)
cd /path/to/staging

# Back up your current config and data
cp qagredo_host/config/config.yaml ./config_backup.yaml
cp -r qagredo_host/data ./data_backup

# Stop running containers
cd qagredo_host && bash run.sh --down && cd ..

# Extract new bundle (overwrites qagredo_host/)
tar xzf qagredo_bundle.tar.gz

# Restore your config and data
cp ./config_backup.yaml qagredo_host/config/config.yaml
cp ./data_backup/* qagredo_host/data/

# Re-run setup (skip image loading if images haven't changed)
cd qagredo_host
bash setup_offline.sh --skip-images

# Run
bash run.sh
```

---

## Further reading

These docs are also in this directory:

| Document | Description |
|----------|-------------|
| `docs/ONLINE_SETUP_GUIDE.md` | Step-by-step guide for the online/dev machine and creating offline bundles |
| `docs/VISUAL_REPORT.html` | **Visual report** with diagrams -- open in any browser |
| `docs/ALGORITHM_REPORT.md` | Full algorithm details, design decisions, and rationale for question generation (10 types), answer generation (structured + 3 retries), and grading (hybrid semantic + LLM-as-judge with Qwen) |
| `docs/OFFLINE_SETUP_GUIDE.md` | Full 5-file offline deployment guide |
| `README.md` | Project overview and architecture |
